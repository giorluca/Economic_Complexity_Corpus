Complexity in Economic and Social Systems

 Complexity in Economic and Social Systems   •   Stanisław Drożdż, Jarosław Kwapień and Paweł OświęcimkaComplexity in Economic and Social SystemsPrinted Edition of the Special Issue Published in Entropywww.mdpi.com/journal/entropyStanisław Drożdż, Jarosław Kwapień and Paweł Oświęcimka Edited byComplexity in Economic and Social SystemsComplexity in Economic and Social SystemsEditorsStanisław Drożdż Jarosław Kwapień Paweł OświęcimkaMDPI • Basel • Beijing • Wuhan • Barcelona • Belgrade • Manchester • Tokyo • Cluj • TianjinEditorsStanisław DrożdżPolish Academy of SciencesPolandJarosław KwapieńPolish Academy of SciencesPolandPaweł OświęcimkaPolish Academy of Sciences PolandEditorial OfficeMDPISt. Alban-Anlage 664052 Basel, SwitzerlandThis is a reprint of articles from the Special Issue published online in the open access journalEntropy (ISSN 1099-4300) (available at: https://www.mdpi.com/journal/entropy/special issues/complexity economic social).For citation purposes, cite each article independently as indicated on the article page online and asindicated below:LastName, A.A.; LastName, B.B.; LastName, C.C. Article Title. Journal Name Year, Volume Number,Page Range.ISBN 978-3-0365-0794-1 (Hbk)ISBN 978-3-0365-0795-8 (PDF)© 2021 by the authors. Articles in this book are Open Access and distributed under the CreativeCommons Attribution (CC BY) license, which allows users to download, copy and build uponpublished articles, as long as the author and publisher are properly credited, which ensures maximumdissemination and a wider impact of our publications.The book as a whole is distributed by MDPI under the terms and conditions of the Creative Commonslicense CC BY-NC-ND.ContentsAbout the Editors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ixStanisław Drożdż, Jarosław Kwapie ń, and Paweł Oświęcimka Complexity in Economic and Social SystemsReprinted from: Entropy 2021, 23, 133, doi:10.3390/e23020133 . . . . . . . . . . . . . . . . . . . . . 1Jarosław Klamut, Ryszard Kutner, and Zbigniew R. StruzikTowards a Universal Measure of ComplexityReprinted from: Entropy 2020, 22, 866, doi:10.3390/e22080866 . . . . . . . . . . . . . . . . . . . . . 5Aleksander JakimowiczThe Role of Entropy in the Development of EconomicsReprinted from: Entropy 2020, 22, 452, doi:10.3390/e22040452 . . . . . . . . . . . . . . . . . . . . 35Roy Cerqueti, Giulia Rotundo and Marcel AusloosTsallis Entropy for Cross-Shareholding Network ConfigurationsReprinted from: Entropy 2020, 22, 676, doi:10.3390/e22060676 . . . . . . . . . . . . . . . . . . . . . 61Pietro Murialdo, Linda Ponta and Anna CarboneLong-Range Dependence in Financial Markets:A Moving Average Cluster Entropy ApproachReprinted from: Entropy 2020, 22, 634, doi:10.3390/e22060634 . . . . . . . . . . . . . . . . . . . . . 77Andrés Garcı́a-Medinaand José B. Hernández C.Network Analysis of Multivariate Transfer Entropy of Cryptocurrencies in Times of TurbulenceReprinted from: Entropy 2020, 22, 760, doi:10.3390/e22070760 . . . . . . . . . . . . . . . . . . . . . 97Stanisław Drożdż, Jarosław Kwapień, Paweł Oświęcimka, Tomasz Stanisz and Marcin WątorekComplexity in Economic and Social Systems: Cryptocurrency Market at around COVID-19Reprinted from: Entropy 2020, 22, 1043, doi:10.3390/e22091043 . . . . . . . . . . . . . . . . . . . . 115Peng Yue, Yaodong Fan, Jonathan A. Batten and Wei-Xing ZhouInformation Transfer between Stock Market Sectors: A Comparison between the USA andChinaReprinted from: Entropy 2020, 22, 194, doi:10.3390/e22020194 . . . . . . . . . . . . . . . . . . . . . 141Gang Chu, Xiao Li, Dehua Shen and Yongjie ZhangUnexpected Information Demand and Volatility Clustering of Chinese Stock Returns: Evidencefrom Baidu IndexReprinted from: Entropy 2020, 22, 44, doi:10.3390/e22010044 . . . . . . . . . . . . . . . . . . . . . 155Zhiqiang Hu, Yuan Hu, Yushan Jiang and Zhen PengPricing Constraint and the Complexity of IPO Timing in the Stock Market: A DynamicGame AnalysisReprinted from: Entropy 2020, 22, 546, doi:10.3390/e22050546 . . . . . . . . . . . . . . . . . . . . . 165Lucia Inglada-PerezA Comprehensive Framework for Uncovering Non-Linearity and Chaos in Financial Markets:Empirical Evidence for Four Major Stock Market IndicesReprinted from: Entropy 2020, 22, 1435, doi:10.3390/e22121435 . . . . . . . . . . . . . . . . . . . . 183vBarbara Będowska-Sójka and Krzysztof EchaustDo Liquidity Proxies Based on Daily Prices and Quotes Really Measure Liquidity?Reprinted from: Entropy 2020, 22, 783, doi:10.3390/e22070783 . . . . . . . . . . . . . . . . . . . . 209Yong Shi, Yuanchun Zheng, Kun Guo, Zhenni Jin and Zili HuangThe Evolution Characteristics of Systemic Risk in China’s Stock Market Based on a DynamicComplex NetworkReprinted from: Entropy 2020, 22, 614, doi:10.3390/e22060614 . . . . . . . . . . . . . . . . . . . . 231Zhen Peng and Changsheng HuThe Threshold Effect of Leveraged Trading on the Stock Price Crash Risk: Evidence from ChinaReprinted from: Entropy 2020, 22, 268, doi:10.3390/e22030268 . . . . . . . . . . . . . . . . . . . . . 247Katarzyna Bień–BarkowskaLooking at Extremes without Going to Extremes:A New Self-Exciting Probability Modelfor Extreme Losses in Financial MarketsReprinted from: Entropy 2020, 22, 789, doi:10.3390/e22070789 . . . . . . . . . . . . . . . . . . . . 261Xin Yang, Xian Zhao, Xu Gong, Xiaoguang Yang and Chuangxia HuangSystemic Importance of China’s Financial Institutions: A Jump Volatility Spillover NetworkReviewReprinted from: Entropy 2020, 22, 588, doi:10.3390/e22050588 . . . . . . . . . . . . . . . . . . . . 287Michał Cieśla and Małgorzata SnarskaA Simple Mechanism Causing Wealth ConcentrationReprinted from: Entropy 2020, 22, 1148, doi:10.3390/e22101148 . . . . . . . . . . . . . . . . . . . . 303Jagoda Kaszowska-Mojsa and Mateusz PipieńMacroprudential Policy in a Heterogeneous Environment—An Application of Agent-BasedApproach in Systemic Risk ModellingReprinted from: Entropy 2020, 22, 129, doi:10.3390/e22020129 . . . . . . . . . . . . . . . . . . . . . 319Aleksander Jakimowicz and Daniel RzeczkowskiInnovativeness of Industrial Processing Enterprises and Conjunctural MovementReprinted from: Entropy 2020, 22, 1177, doi:10.3390/e22101177 . . . . . . . . . . . . . . . . . . . . 393Eduardo Viegas, Hayato Goto, Yuh Kobayashi, Misako Takayasu, Hideki Takayasu andHenrik Jeldtoft JensenAllometric Scaling of Mutual Information in Complex Networks: A Conceptual Frameworkand Empirical ApproachReprinted from: Entropy 2020, 22, 206, doi:10.3390/e22020206 . . . . . . . . . . . . . . . . . . . . 427Aleksander Jakimowicz and Daniel RzeczkowskiNew Measure of Economic Development Based on the Four-Colour TheoremReprinted from: Entropy 2021, 23, 61, doi:10.3390/e23010061 . . . . . . . . . . . . . . . . . . . . . 441Bedane S. Gemeda, Birhanu G. Abebe, Andrzej Paczoski, Yi Xie and Giuseppe T. CirellaWhat Motivates Speculators to Speculate?Reprinted from: Entropy 2020, 22, 59, doi:10.3390/e22010059 . . . . . . . . . . . . . . . . . . . . . 473José Martins and Alberto PintoThe Value of Information Searching against Fake NewsReprinted from: Entropy 2020, 22, 1368, doi:10.3390/e22121368 . . . . . . . . . . . . . . . . . . . . 491viDorian Wild, Margareta Jurcic and Boris PodobnikThe Gender Productivity Gap in Croatian Science: Women Are Catching up with MalesandBecoming Even BetterReprinted from: Entropy 2020, 22, 1217, doi:10.3390/e22111217 . . . . . . . . . . . . . . . . . . . . 509viiAbout the EditorsStanisław Drożdż, Professor, Present professor of physics and head of the Complex Systems Theory Department at Institute of Nuclear Physics (INP), Polish Academy of Sciences and professor of computer science at the Cracow University of Technology. He received an M.S. (1978) and Ph.D. (1982) in physics, both from Jagiellonian University in Kraków, and a DSc (1988) in theoretical physics from INP. In 1994, he received the Polish state title of professor. His long-term scientific studies abroad include Forschungszentrum Juelich, Germany from 1983 to 1986 as a post-doc and from 1989 to 1991 as a senior scientist, as well as the University of Illinois at Urbana-Champaign, USA from 1993 to 1994 and Bonn University, Germany from 2001 to 2002 as a visiting professor. His research interests and activities include the quantum many-body problem, nonlinear dynamics, general aspects of complexity, brain-research-related issues, dynamics of financial markets, and quantitative linguistics.Jarosław Kwapień , Assoc. Prof., Graduated from Jagiellonian University in Kraków in 1995 with a Master’s Degree in physics. He started his doctoral programme at the Institute of Nuclear Physics (INP), Polish Academy of Sciences and earned a Ph.D. degree in 2001 based on his work on topics at the interface between statistical physics and neuroscience. As a post-doctoral fellow at Forschungszentrum Juelich, Germany, he started his research on topics in the physics of financial markets (econophysics) and data science. Later, he entered a tenure track at INP and completed his habilitation in 2010, after which he obtained the position of Associate Professor. Recently, he has continued to publish works in econophysics, but also began an interest in complex networks, quantitative linguistics, and other interdisciplinary topics.Paweł Oświęcimka, Assoc. Prof., Paweł Oświecimka received an M.S. degree in physics from the AGH University of Science and Technology, Kraków, Poland, in 2001, and Ph.D. and D.Sc. degrees in physics from Institute of Nuclear Physics (INP), Polish Academy of Sciences, Kraków, in 2005 and 2015, respectively. Since 2016, he has been Associate Professor at the Complex Systems Theory Department INP. He also leads one of the research groups in the project on bio-inspired artificial neural networks at the Jagiellonian University and he is a lecturer at the Cracow University of Technology. His current research interests include interdisciplinary research in natural and social complex systems by means of advanced methods of time series analysis, with particular emphasis on multifractal analysis, complex network analysis, and modelling of multi-scaling processes. He is a Fellow of the Commission of Complex Systems at the Polish Academy of Arts and Sciences (PAU).ixentropyEditorialComplexity in Economic and Social SystemsStanisław Drożdż 1,2,*, Jarosław Kwapień 1 and Paweł Oświęcimka 1,31 Complex Systems Theory Department, Institute of Nuclear Physics, Polish Academy of Sciences,ul. Radzikowskiego 152, 31-342 Kraków, Poland; jaroslaw.kwapien@ifj.edu.pl (J.K.);pawel.oswiecimka@ifj.edu.pl (P.O.)2 Faculty of Computer Science and Telecommunication, Cracow University of Technology, ul. Warszawska 24,31-155 Kraków, Poland3 Faculty of Physics, Astronomy and Applied Computer Science, Jagiellonian University, ul. Prof. StanisławaŁojasiewicza 11, 30-348 Kraków, Poland* Correspondence: stanislaw.drozdz@ifj.edu.plReceived: 5 January 2021; Accepted: 18 January 2021; Published: 21 January 2021During recent years we have witnessed a systematic progress in the understanding of complexsystems, both in the case of particular systems that are classified into this group and, in general,as regards the phenomenon of complexity [1]. This is possible owing to an outburst of research interestin the science of complexity and a joint effort made by the researchers representing different disciplinesand backgrounds which resulted in the enormous number of interdisciplinary studies carried out. Thisprogress has been achieved on both the theoretical, model, and experimental levels. However, in orderto comprehend the complexity in full detail, much is still to be done. This is particularly true in thecase of the systems involving human society and behaviour.This Special Issue of Entropy was intended to attract researchers specializing in interdisciplinarystudies of complex systems, with the economic and social systems in particular, and to collect inone place their contributions that otherwise could be scattered among many journals and issues. Webelieve that the papers spanning this issue can be considered as valuable input to their specific fields,but also to complexity science in general. Some of them because they relate to general concepts andthus their conclusions can be exploited in various situations across many fields and others because ofthe methods that were used there, the knowledge of which can be disseminated more broadly. We areglad that our idea was met with a positive response and now we can present as many as 23 genuineresearch papers on a wide spectrum of topics. The largest set of papers is related to the economicalsystems, while smaller sets to the social systems and to general complexity, with such a proportionreflecting the total amount of the current scientific output in these fields.Complexity still lacks a commonly accepted strict definition. In many practical cases it suffices tounderstand this notion intuitively as a nontrivial, irreducible order (i.e., other than simple regularity ora straightforward effect of a lower level of organization) that spontaneously emerges from an overallchaos [1], but there is also a strong need to provide a strict definition that, for instance, can be appliedto categorize various systems based on their structure and dynamics or to construct a measure ofcomplexity. There were a plenty of attempts in that direction but they largely failed. An interestingstep towards resolving this issue is presented in a paper [2] where its authors propose a measure ofcomplexity based on a nonlinear transformation of time-dependent entropy that attributes the highestcomplexity to the optimally mixed states between maximum regularity and maximum disorder.Various tools based on entropy are frequently used in the context of complex systems and it isnot surprising that they are applied in a few other contributed papers. One of the principal directionsof research is looking for precursors of the oncoming structural phase transitions. For example,transfer entropy quantifying dependence asymmetry between two systems is used to construct anetwork of information transfers among cryptocurrencies. The resulting network topology revealssignificant alteration during turmoils and forecasts a systemic risk increase [3]. The Tsallis nonextensiveentropy has already proved useful in studying complex systems [4]. It is applied to analyze theEntropy 2021, 23, 133; doi:10.3390/e23020133 www.mdpi.com/journal/entropy1Entropy 2021, 23, 133cross-shareholding networks of companies. In this context it offers a measure of market polarisationand a tool for analyzing market self-organization in response to external shocks [5]. Finally, the movingaverage cluster entropy is proposed to study the long-range dependence in time series and provesuseful as a measure capturing endogenous sources of risk over different temporal horizons [6].Risk, which quantifies market or asset stability and vulnerability to external shocks, has alwaysbeen one of the key topics in economics, but it is also an important issue from the complexityperspective [7]. A few more papers from this Special Issue consider systemic risk as one of theircentral points. If such a risk is quantified in terms of some framework, it is possible to observe itsevolution on a given market. For example, the Chinese stock market network topology analysis leadsto a conclusion that the systemic risk can be decomposed into a clear trend and periodic fluctuationswith the former reflecting the gradual improvement of the management and operation of the marketand the latter reflecting the events of excessive strength [8]. Among the most important sources of riskis leverage trading but this relation can rather be non-trivial with either stabilizing or destabilizingimpact depending on the leverage trading share in total market activity [9]. In order to manage risk,one needs to construct realistic models that are able to predict the probability of financial losses [10]and to use reliable measures able to provide one with sufficiently early alerts [3]. On the other hand,risk can also be managed by identifying the key companies or sectors that are its sources of centers [11].Financial markets are among the most interesting complex systems from a perspective ofthe 
empirical data analysis, because they provide incomparably clean data. This is why mucheffort dedicated to studying these markets can be fruitful far beyond the field of economics [1].Self-organization of the stock markets and their hierarchical structure can be approached from theangle of information transfer between different sectors in various time intervals [12]. This also refers tothe cross-shareholding market structure which self-organizes under the influence of external shocks [5].Both the external shocks and the internal market events can produce excessive demand for information,which, if properly quantified, may offer a way to monitor oncoming market events that are difficult topredict by using other methods. A new tool is proposed based on an internet search engine like theChinese Baidu [13].Another signature of market complexity is pricing and timing of the stocks during their initialpublic offerings. In [14], a few results on the IPO timing properties are considered and discussed.Studying the temporal properties of financial dynamics, which is a property related to their complexity,offers verification for one of the key paradigms of financial markets, namely the efficient markethypothesis [6], and testing for nonlinearity and chaos [15]. Temporal properties of financial dynamicscan in turn be one of the consequences of stock liquidity and it is thus important to have a reliablemethod to quantify it [16].Same as the most financial markets offer high quality data, the cryptocurrency market offers alsoa unique possibility of observation of the whole process of new complex system development fromscratch [17]. The cryptocurrency market is also interesting as the possible future fate of money. So theirevolution and reacting to external shocks like the COVID-19 pandemic is particularly interesting andinstructive [3], especially as it seems that this market gradually reaches maturity [18].Modelling of the economical systems can go beyond the financial markets and also be applied tomore general problems like the wealth condensation in society (simple but effective an agent-basedmodel in [19]), the innovation-related performance on a market (the innovation pressure effects ofprivate-owned enterprises and public companies [20]), and the impact of the macroprudential policyon economy and the financial system (with the results on the stabilizing effects of such policy duringthe turmoils and crises [21]). From this general system level one may look downwards into the systemcomponent parts, which reveal complexity on their own: the geographical regions or administrativedivisions. It is possible to quantify their economical development by a newly proposed method of thepublic administration website quality assessment [22] and to analyze differences in an inter-regionalbusiness ecosystem structure or economic activity efficiency level by means of a network approach [23].2Entropy 2021, 23, 133Complex phenomena occurring on the interface between economical activity and spatial structureare the subject of a study of land speculation on the outskirts of a sample city in Ethiopia [24]. Thisstudy investigates motivations the speculators are driven by and concludes on a possible directionlocal governments should proceed in order to diminish negative impact of such practices on citydevelopment. An even more important social phenomenon with a negative impact on the societyis fake news. A model of rumour spreading with evolutionary information search dynamics allowsone for analyzing optimal search strategies that maximize pay-off for the society and potentiallyprovides the policy makers with the recommendations how to minimize the harmful impact of fakenews [25]. The most sociologically-oriented study of this Special Issue considers the research output ofthe male and female scientists quantified in terms of their publication citations from the perspectiveof the gender productivity gap [26]. It occurs that a larger gender inequality can be found in theSTEM disciplines (i.e., science, technology, engineering, and mathematics) as compared with thenon-STEM ones.Finally, it is worth to mention a more history-oriented essay on the impact of physics (withthermodynamics in particular) on the development of ideas in the contemporary economics [27]. It isan interesting example of the innovation-generating potential of the interdisciplinary cooperation inscience, which is the exceptionally welcome in the science of complex systems.After this introduction, readers are warmly invited to read the papers collected in thisSpecial Issue.Funding: This research received no external funding.Conflicts of Interest: The authors declare no conflict of interest.References1. Kwapień, J.; Drożdż, S. Physical approach to complex systems. Phys. Rep. 2012, 515, 115. [CrossRef]2. Klamut, J.; Kutner, R.; Struzik, Z.R. Towards a Universal Measure of Complexity. Entropy 2020, 22, 866.[CrossRef] [PubMed]3. García-Medina, A.; Hernandéz, C.J.B. Network Analysis of Multivariate Transfer Entropy ofCryptocurrencies in Times of Turbulence. Entropy 2020, 22, 760. [CrossRef] [PubMed]4. Tsallis, C. Introduction to Nonextensive Statistical Mechanics: Approaching a Complex World; Springer: New York,NY, USA, 2009.5. Cerqueti, R.; Rotundo, G.; Ausloos, M. Tsallis Entropy for Cross-Shareholding Network Configurations.Entropy 2020, 22, 676. [CrossRef] [PubMed]6. Murialdo, P.; Ponta, L.; Carbone, A. Long-Range Dependence in Financial Markets: A Moving AverageCluster Entropy Approach. Entropy 2020, 22, 634. [CrossRef] [PubMed]7. Sornette, D. Why Stock Market Crash: Critical Events in Complex Financial Systems; Princeton University Press:Princeton, NJ, USA, 2003.8. Shi, Y.; Zheng, Y.; Guo, K.; Jin, Z.; Huang, Z. The Evolution Characteristics of Systemic Risk in China’s StockMarket Based on a Dynamic Complex Network. Entropy 2020, 22, 614. [CrossRef]9. Peng, Z.; Hu, C. The Threshold Effect of Leveraged Trading on the Stock Price Crash Risk: Evidence fromChina. Entropy 2020, 22, 268. [CrossRef]10. Bień-Barkowska, K. Looking at Extremes without Going to Extremes: A New Self-Exciting Probability Modelfor Extreme Losses in Financial Markets. Entropy 2020, 22, 789. [CrossRef]11. Yang, X.; Zhao, X.; Gong, X.; Yang, X.; Huang, C. Systemic Importance of China’s Financial Institutions: AJump Volatility Spillover Network Review. Entropy 2020, 22, 588. [CrossRef]12. Yue, P.; Fan, Y.; Batten, J.A.; Zhou, W.-X. Information Transfer between Stock Market Sectors: A Comparisonbetween the USA and China. Entropy 2020, 22, 194. [CrossRef]13. Chu, G.; Li, X.; Shen, D.; Zhan, Y. Unexpected Information Demand and Volatility Clustering of ChineseStock Returns: Evidence from Baidu Index. Entropy 2020, 22, 44. [CrossRef] [PubMed]14. Hu, Z.; Hu, Y.; Jiang, Y.; Peng, Z. Pricing Constraint and the Complexity of IPO Timing in the Stock Market:A Dynamic Game Analysis. Entropy 2020, 22, 546. [CrossRef] [PubMed]3Entropy 2021, 23, 13315. Inglada-Perez, L. A Comprehensive Framework for Uncovering Non-Linearity and Chaos in FinancialMarkets: Empirical Evidence for Four Major Stock Market Indices. Entropy 2020, 22, 1435. [CrossRef][PubMed]16. Będowska-Sójka, B.; Echaust, K. Do Liquidity Proxies Based on Daily Prices and Quotes Really MeasureLiquidity? Entropy 2020, 22, 783. [CrossRef] [PubMed]17. Wątorek, M.; Drożdż, S.; Kwapień, J.; Oświęcimka, P.; Stanuszek, M. Multiscale characteristics of theemerging global cryptocurrency market. Phys. Rep. 2020. [CrossRef]18. Drożdż, S.; Kwapień, J.; Oświęcimka, P.; Stanisz, T.; Wątorek, M. Complexity in Economic and Social Systems:Cryptocurrency Market at around COVID-19. Entropy 2020, 22, 1043. [CrossRef]19. Cieśla, M.; Snarska, M. A Simple Mechanism Causing Wealth Concentration. Entropy 2020, 22, 1148.[CrossRef]20. Jakimowicz, A.; Rzeczkowski, D. Innovativeness of Industrial Processing Enterprises and ConjuncturalMovement. Entropy 2020, 22, 1177. [CrossRef]21. Kaszowska-Mojsa, J.; Pipień, M. Macroprudential Policy in a Heterogeneous Environment—An Applicationof Agent-Based Approach in Systemic Risk Modelling. Entropy 2020, 22, 129. [CrossRef]22. Jakimowicz, A.; Rzeczkowski, D. New Measure of Economic Development Based on the Four-ColourTheorem. Entropy 2020, 22, 61. [CrossRef]23. Viegas, E.; Goto, H.; Kobayashi, Y.; Takayasu, M.; Takayasu, H.; Jensen, H.J. Allometric Scaling of MutualInformation in Complex Networks: A Conceptual Framework and Empirical Approach. Entropy 2020, 22,206. [CrossRef] [PubMed]24. Gemeda, B.S.; Abebe, B.G.; Paczoski, A.; Xie, Y.; Cirella, G.T. What Motivates Speculators to Speculate?Entropy 2020, 22, 59. [CrossRef] [PubMed]25. Martins, J.; Pinto, A. The Value of Information Searching against Fake News. Entropy 2020, 22, 1368.[CrossRef]26. Wild, D.; Jurcic, M.; Podobnik, B. The Gender Productivity Gap in Croatian Science: Women Are Catchingup with Males and Becoming Even Better. Entropy 2020, 22, 1217. [CrossRef] [PubMed]27. Jakimowicz, A. The Role of Entropy in the Development of Economics. Entropy 2020, 22, 452. [CrossRef]© 2021 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open accessarticle distributed under the terms and conditions of the Creative Commons Attribution(CC BY) license (http://creativecommons.org/licenses/by/4.0/).4entropyArticleTowards a Universal Measure of ComplexityJarosław Klamut 1, Ryszard Kutner 1,* and Zbigniew R. Struzik 1,2,31 Faculty of Physics, University of Warsaw, Pasteura 5, 02-093 Warsaw, Poland;jaroslaw.klamut@fuw.edu.pl (J.K.); z.r.struzik@p.u-tokyo.ac.jp (Z.R.S.)2 Graduate School of Education, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan3 Advanced Center for Computing and Communication, RIKEN, 2-1 Hirosawa, Wako,Saitama 351-0198, Japan* Correspondence: ryszard.kutner@fuw.edu.plReceived: 27 May 2020; Accepted: 3 August 2020; Published: 6 August 2020Abstract: Recently, it has been argued that entropy can be a direct measure of complexity, where thesmaller value of entropy indicates lower system complexity, while its larger value indicates highersystem complexity. We dispute this view and propose a universal measure of complexity that is basedon Gell-Mann’s view of complexity. Our universal measure of complexity is based on a non-lineartransformation of time-dependent entropy, where the system state with the highest complexity isthe most distant from all the states of the system of lesser or no complexity. We have shown thatthe most complex is the optimally mixed state consisting of pure states, i.e., of the most regular andmost disordered which the space of states of a given system allows. A parsimonious paradigmaticexample of the simplest system with a small and a large number of degrees of freedom is shownto support this methodology. Several important features of this universal measure are pointed out,especially its flexibility (i.e., its openness to extensions), suitability to the analysis of system criticalbehaviour, and suitability to study the dynamic complexity.Keywords: dynamical complexity; universal complexity measure; irreversible processes; entropies;entropic susceptibilities1. IntroductionAnalysis of the concept of complexity is a non-trivial task due to its diversity, arbitrariness,uncertainty, and contextual nature [1–10]. There are many different levels/scales, faces, and typesof complexity, researched with very different technologies/techniques and tools [11–13] (and refs.therein). In the context of dynamical systems, Grassberger suggested [14] that a slow convergence of theentropy to its extensive asymptotic limit is a signature of complexity. This idea was materialized [15,16]further by information and statistical mechanics techniques. It generalizes many previous approachesto complexity, unifying physical ideas with ideas from learning and coding theory [17]. There alsoexists a connection of this approach to algorithmic or Kolmogorov complexity. The hidden patterncan be the essence of complexity [18–21]. Techniques adapted from the theories of informationand computation have led physical science (in particular, the region extended between classicaldeterminism and deterministic chaos) to discover hidden patterns and quantify their dynamicstructural complexity [22]. The above approaches are not universal—they only capture small fragmentsof the concept of complexity.We must remember that complexity also depends on the conditions imposed (e.g., boundary orinitial conditions), as well as the restrictions adopted. This creates a challenge for every complexitystudy. It concerns the complexity that can appear in the movement of a single entity and collection ofentities braided together. These entities can be irreducible or straightforward, simple systems, but theycan also be complex systems.Entropy 2020, 22, 866; doi:10.3390/e22080866 www.mdpi.com/journal/entropy5Entropy 2020, 22, 866When we talk about complexity, we mean irreducible complexity, which can no longer be dividedinto smaller sub-complexities. We refer to this as a primary complexity. Considering the primarycomplexity here, we mean one that can be expressed at least in an algorithmic way—it is an effectivecomplexity if it also contains a logical depth [23–27]. We should take into account that our models(analytical and numerical) and theories describing reality are not fully deterministic. The evolution ofa complex system is potentially multi-branched and the selection of an alternative trajectory (or branchselection) is based on decisions taken randomly.One of the essential questions concerning a complex system is the problem of its stability/robustnessand the question of the stationarity of its evolution [28]. Moreover, the relationship betweencomplexity and disorder on the one hand, and complexity and pattern on the other is an importantquestion—especially in the context of irreversible processes, where non-linear processes, running awayfrom the equilibrium, play a central role. Financial markets can be a spectacular example of theseprocesses [29–39].The central question of whether entropy is a direct measure of complexity is one we answer in thenegative. In our opinion, based on the Gell–Mann concept of complexity, the measure of complexity isappropriately, non-linearly transformed entropy. This work is devoted to finding this transformationand examining the resulting consequences.2. Definition of a Universal Measure of Complexity and Its PropertiesIn this Section, we translate the Gell–Mann general qualitative concept of complexity into thelanguage of mathematics, and we present the consequences of this.2.1. The Gell–Mann Concept of ComplexityThe problem of defining a universal measure of complexity is urgent. For this work, the Gell–Mannconcept [23,40] of complexity is the inspirational starting point. We apply this concept to irreversibleprocesses, by assuming that both fully ordered and fully disordered systems cannot be the complex.The fully ordered system essentially has no complexity because of maximal possible symmetryof the system, but the fully disordered system contains no information as it entirely dissipates.Hence, the maximum of complexity should be sought somewhere in between these pure extreme states.This point of view allows for the introduction of a formal quantitative phenomenological complexitymeasure based on entropy as a parameter of order [29,41]. This measure reflects the dynamics of thesystem through the dependence of entropy on time. The vast majority of works analyzing the generalaspects of complexity, including its basis, are based on information theory and computational analysis.Such an approach requires supplementing with a provision allowing a return from a bit representationto physical representation—only this will allow physical interpretations, including understanding ofthe causes of complexity.We define the phenomenological partial measure of complexity as a non-linear function of entropyS of the order of (m, n),CX(S; m, n) def.= (Smax − S)m(S − Smin)n= CX(S; m − 1, n − 1)[(Z2)2− (S − Sarit)2], m, n ≥ 1, (1)where Smin and Smax are minimal and maximal values of entropy S, respectively, Sarit = Smin+Smax2 ,and the entropic span Z def.= Smax − Smin, whereas m and n are natural numbers (an extension to realpositive numbers is possible but this is not the subject of this work). 
They define the order (m, n) ofthe partial measure of complexity CX. Let us add that this formula is also applicable at a mesoscopicscale. In other words, complexity appears in all systems for which we can build entropy. Notably, Smaxdoes not have to concern the state of thermodynamic equilibrium of the system. It may refer to the6Entropy 2020, 22, 866state for which entropy reaches its maximum value in the observed time interval. However, in thiswork, we are only limited to systems having a state of thermodynamic equilibrium. Below, we discussthe Equation (1), indicating that it satisfies all properties of the measure of complexity. Of course,when m = 0 and n = 1 then CX simply becomes S − Smin, i.e., the entropy of the system (the constantis not important here). However, when m = 1, n = 0, we obtain the information contained in thesystem (constant does not play a role here). Equation (1) gives us a lot more—showing this is thepurpose of this work (helpful features of CX are shown in Appendix A).The partial measure of complexity given by Equation (1) is determined with the accuracy ofthe additive constant of S, i.e., this constant does not contribute to the measure of the complexity ofthe system.Using Equation (1), we can also enter the partial measure of specific complexity, as follows,cx(s; m, n) def.=1Nm+nCX(Ns; m, n), (2)where N is the number of entities that make up the system and specific entropy s = S/N. As one cansee, the partial measure of specific complexity cx is independent of N for an extensive system. Specificentropy and specific complexity are particularly convenient when comparing different extensivesystems and when we do not examine the complexity dependence on N.However, the extraction of an additional multiplicative constant (e.g., particle number) to haves independent of N often presents a technical difficulty, or may even be impossible, especially fornon-extensive systems. Then it is more convenient to use the entropy of the system instead of thespecific entropy. It is also important to realize that determining extreme entropy values (or extremespecific entropy values) of actual systems can be complicated and it requires additional dedicatedtools/technologies, algorithms, and models.The partial measures of complexity are enslaved by entropy in every order (m, n) of complexity.However, the kind of entropy we use in Equation (1) depends on the specific situation of the systemand what we want to know about the system, because our definition of complexity does not specifythis. From our point of view, relative entropies formulated in the spirit of Kullback–Leibler seem to bethe most appropriate (this is referred to in Appendix B). Using the Kullback–Leibler type of entropy,one can express both ordinary entropies and conditional entropies, in particular one can describe theentropy rate increasingly used in the context of complexity analysis.The entropy here can be both additive (the Boltzmann–Gibbs thermodynamic one [42], Shanoninformation [17], Rényi [43]), and non-additive entropy (Tsallis [44]). The measure CX(S) is a concave(or convex up) function of entropy S, which disappears on the edges at points S = Smin and S = Smax.It has a maximumCXmax = CX(S = SmaxCX ; m, n) = mm nn(Zm + n)m+n(3)at pointS = SmaxCX = S =mSmin + nSmaxm + n=1m Smax + 1n Smin1m +1n(4)as at this point dCX(S)dS |S=S = 0 and d2CX(S)dS2 |S=S < 0. The quantity SmaxCX is a characteristic alsobecause it is a weighted average. The quantity CXmax is well suited to global universal measurementsof complexity, because (at a given order (m, n)), it only depends on the entropy span Z. The quantitycxmax def.= CXmax/Nm+n might also be a good candidate for measuring the logic depth of complexity.7Entropy 2020, 22, 8662.2. The Most Complex StructureThe question now arises about the structure of the system corresponding to entropy SmaxCX givenby Equation (4). The answer is given by the following constitutive equation,S(Y = YCXmax)= SmaxCX , (5)where Y is the set of variables and parameters (e.g., thermodynamic), on which the state of the systemdepends. However, Y = YCXmaxis a set of such values of these variables and parameters that are thesolution of Equation (5). This solution gives the entropy value S = SmaxCX that maximizes the partialmeasure of complexity, that is CX = CXmax. Hence, with the value of YCXmax, we can finally answer thekey question: what structure/pattern is behind CXmax or what the structure of maximum complexitylooks like.There are a few comments to be made regarding the constitutive Equation (5) itself. It is a(non-linear) transcendental equation in the untangled form relative to the Y. This equation shouldbe numerically solved, because we do not expect it to have an analytical solution for maximallycomplex systems. An instructive example of a specific form of this equation and its solution for aspecific physical problem is presented in Section 3. However, this will help us to understand how ourmachinery works.Equation (4) legitimizes the measure of complexity we have introduced. Namely, its maximumvalue falls on the weighted average entropy value, which describes the optimal mixture of completelyordered and completely disordered phases. To the left of S, we have a phase with dominance oforder and to the right a phase with dominance of disorder. The transition between both phases atS is continuous. Thus, we can say that the partial measure of complexity that we have introducedalso defines a certain type of phase diagram in S and CX variables (phase diagram plain). Section 2.5provides more detailed information.2.3. Evolution of the Partial Measure of ComplexityDifferentiating Equation (1) over time t, we obtain the following non-linear dynamics equation,dCX(S(t); m, n)dt= χCX(S; m, n)dS(t)dt= (m + n) [SmaxCX − S(t)]CX(S(t); m − 1, n − 1)dS(t)dt, (6)where the entropic S-dependent (non-linear), susceptibility is defined byχCX(S; m, n)def.=∂CX(S; m, n)∂S= (m + n) [SmaxCX − S(t)]CX(S(t); m − 1, n − 1) (7)and dS(t)dt can be expressed, for example, using the right-hand side of the master Markov equation(see Ref. [45] for details). However, we must realize that the dependence of entropy on time can,in general, be non-monotonic, because real systems are not isolated (cf. the schematic plot in Figure 2).One can see how the dynamics of complexity is controlled in a non-linear way by the evolution of theentropy of the system.In concluding this Section, we state that Equations (1)–(6) together provide a technology forstudying the multi-scale aspects of complexity, including the dynamic complexity. However, it is still asimplified approach, as we show in Section 4.2.4. Significant Partial Measure of ComplexityWe consider the partial measure of complexity to be significant when the entropy of the systemis located between two inflection points of the CX(S; m, n) curve, i.e., in the range S−ip ≤ S ≤ S+ip.This case occurs for n, m ≥ 2. We then obtain8Entropy 2020, 22, 866Smin < S∓ip = Smin +n(n − 1)√n(n + m − 1)Smax − Smin√n(n + m − 1)±√m < Smax, (8)see Figure 1d for details.There are two different cases where a single inflection point is present. Namely,Smin < S−ip =2Smax + m(m − 1)Smin2 + m(m − 1) < S, for m ≥ 2, n = 1, (9)andS < S+ip =2Smin + n(n − 1)Smax2 + n(n − 1) < Smax, for m = 1, n ≥ 2. (10)In Figure 1b, we present the case defined by Equation (9), while that defined by Equation (10) isshown in Figure 1c.For n = m = 1, the curve CX(S; m, n) vs. S has no inflection points and it looks like a horseshoe(cf. Figure 1a).Notably, we can equivalently writeSmin < S∓ip = Smax − m(m − 1)√m(n + m − 1)Smax − Smin√m(n + m − 1)∓√n < Smax, for n, m ≥ 2. (11)Let us consider the span Zip = S+ip − S−ip of the two-phase area. From Equation (8), or equivalentlyfrom Equation (11), we obtainZip =2√nm(n + m)√n + m − 1 Z. (12)As one can see, the span Zip depends linearly on the span Z and in a non-trivial way on theexponents n and m. Thus, with the Z set, only Zip’s non-trivial dependence on the order (m, n) ofmeasure of complexity CX occurs, which is different from CXmax dependence. In other words, Zip isless sensitive to complexity than CXmax.The significant partial measure of complexity ranges between the two inflection points only forthe case n, m ≥ 2 (cf. Figure 1d). Indeed, a mixture of phases is observed in this area. For areas whereSmin ≤ S < S−ip and S+ip < S ≤ Smax, we have (practically speaking) only single phases, ordered anddisordered, respectively (see Section 2.5 for details). The case defined by Equation (8), and equivalentlyby Equation (11), is the most general, while taking into account the fullness of complexity behaviouras a function of entropy. Other cases impoverish the description of complexity. Therefore, we willcontinue to consider the situation, where n, m ≥ 2.The choice of any of the CX(S; m, n) forms (i.e., exponents n and m) is a somewhat arbitraryfunction of the state of the system as it depends on the function of the state, that is on the entropy. In ouropinion, the shape of the CX(S; m, n) measure vs. S we present in Figure 1d is the most appropriate,because only then the significant complexity is ranging between non-vanishing inflection points S−ipand S+ip.In generic case we should, however, use the series of partial measures defined by Equation (1).Then, we define the order of the partial complexity using the pair of exponents (n, m). The introductionof the order of the partial complexity is in line with our perception of the existence of multiple levelsof (full) complexity.We are able to discover the nature of the CX measure, i.e., its dynamics and, in particular,its dynamical structures, when we analyze the entropy dynamics S(t) (see Figure 2 for details).9Entropy 2020, 22, 866Figure 1. Plots of the partial measure of complexity CX(S; m, n) vs. S given by Equation (1) for fourcharacteristic cases: (a) Case n = m = 1 where no inflection points, S∓ip are present. (b) Case m = 2 andn = 1 where a single inflection point S+ip is present. (c) Case m = 1 and n = 2 where a single inflectionpoint S−ip is present. (d) Case m = 2 and n = 2 where both inflection points are present. The shape ofthe curve, containing two inflection points, is typical for partial measures of complexity, characterizedby exponents m, n ≥ 2. Numbers 1–4 mark individual phases differing in the degree of order.Figure 2. Schematic plot of the partial measure of complexity CX(S; m, n) vs. S and t given byEquation (1). The red curve shows the dependence of entropy S on time t. The black curve representsCX(S(t); m, n) in three dimensions. The blue curve represents projection of the black curve on the(S, CX) plane. We show different variants of this blue curve presented in Figure 1. The non-monotonicdependence of the entropy on time visible here indicates the open nature of the system. However,this non-monotonicity is not visible through the blue curve. For instance, the three local maxima of theblack curve collapse to one of the blue curve.The measurability of the partial measure of complexity is necessary for characterizing it quantitativelyand to be able to compare different complexities. Following Gell–Mann [40], we must identify the10Entropy 2020, 22, 866scales at which we perform the analysis and thus determine coarse-graining to define the entropy.Its dependence on complexity cannot be ruled out.However, the question of direct measurement of the partial measure of complexity in anexperiment (real or numerical) remains a challenge.2.5. Remarks on the Partial Entropic SusceptibilityAn essential tool for studying phase transitions is the system susceptibility—in our case, the partialentropic susceptibility of the partial measure of complexity. Here, it (additionally) plays the role of thepartial order parameter.The plot of susceptibility χCX(S; m, n) vs. S is presented in Figure 3. Four phases, already markedin Figure 1, are visible (also numbered 1 to 4).Figure 3. Plot of the partial entropic (non-equilibrium) susceptibility χCX(S; m, n) of the partial measureof complexity vs. S given by Equation (7) at fixed order (m = 2, n = 2). The finite susceptibilityvalue at the S−ip and S+ip phase transition points (cf. Figure 1) may be considered to correspond to finitesusceptibility value in absorbing the non-equilibrium phase transition in the model of direct percolationat a critical point in the presence of an external field [21]. However, the situation presented here isricher, because susceptibility changes its sign, smoothly passing through zero at S = SmaxCX . At thispoint, the system is exceptionally robust and, therefore, is poorly affected by data artefacts, because itssusceptibility vanishes there.Phase number 1 is almost entirely ordered—the disordered phase input is residual. At point S−ip,there is a phase transition to the mixed-phase marked with number 2, still with the predominance ofthe ordered phase. At the S−ip inflection point, the entropic susceptibility reaches a local maximum.By further increasing the entropy of the system, it enters phase 3 as a result of phase transition at thevery specific SmaxCX transition point. At this point, the entropic susceptibility of the partial measure ofcomplexity disappears. This mixed phase (number 3) is already characterized by the advantage ofthe disordered phase over the ordered one. Finally, the last transition, which occurs at S+ip, leads thesystem to the dominating phase of the disordered phase—the input of the ordered phase is residualhere. At this transition point, the susceptibility reaches a local minimum. Intriguingly, entropic11Entropy 2020, 22, 866susceptibility can have both positive and negative value passing smoothly through zero at S = SmaxCX ,where the system is exceptionally robust. The presence of phases with positive and negative entropicsusceptibility is an exceptionally intriguing phenomenon. The phases discussed above, together withthe above-mentioned inflection points, are also marked in Figure 1d. Let us add that the location of thephases mentioned above, i.e., the location of the inflection points, depends on the order (m, n) of thepartial measure of complexity. This is clearly seen in Figures 4 and 5.The values of local extremes of the entropic susceptibility of the partial measure of complexity arefinite here and not divergent, as in the case of (equilibrium and non-equilibrium) phase transitions inthe absence of an external field. We use this definition to describe the critical behaviour of a systemthat we demonstrate in Section 2.7, where it requires an explicit dependence on N.2.6. Universal Full Measure of ComplexityThe full universal measure of complexity X is a weighted sum of the partial measures ofcomplexity CX(S; m, n) for individual scales. That is,X(S; m0, n0) = ∑m≥m0,n≥n0w(m, n)CX(S; m, n), m0, n0 ≥ 0, (13)where w(m, n) is a normalized weight, which must be given in an explicit form. This form is to someextent imposed by the power-law form of partial complexity. Namely, we can assumew(m, n) =(1 − 1M)2 1Mm−m0+n−n0 , M > 1, (14)which seems to be particularly simple becausew(m + 1, n)w(m, n)=w(m, n + 1)w(m, n)=1M, (15)independently of m and n.As one can see, Equation (13), supported by Equation (15), is the product of the sums of twogeometric series,X(S; m0, n0) = (Smax − S)m0(1 − 1M)∑m≥m0(Smax − S)m−m0Mm−m0× (Smax − S)n0(1 − 1M)∑n≥n0(Smax − S)n−n0Mn−n0 . (16)If both series converge for any Smin ≤ S ≤ Smax, which is the case if and only if the conditionZ(= Smax − Smin) < M is met, then we directly obtainX(S; m0, n0) =(1 − 1M)2 (Smax − S)m01 − Smax−SM(S − Smin)n01 − S−SminM. (17)In other words, the M parameter can always be chosen, so that the sums of both series inEquation (21) diverge for all S values. Thus, m0, n0 ≥ 1 is the natural lower limit of m0, n0, satisfyingthe condition of X(S; m0, n0) disappearing for S = Smin, Smax. We still assume more strongly thatm0, n0 ≥ 2, which has already been explained above.For extensive systems, Equation (17) can be presented in a form that clearly shows the dependenceof the X complexity on the number of entities N, simply replacing S entropy by Ns, where s is alreadyN-independent specific entropy. Subsequently,12Entropy 2020, 22, 866X(Ns; m0, n0) =(1 − 1M)2Nm0+n0(smax − s)m01 − NM (smax − s)(s − smin)n01 − NM (s − smin). (18)We emphasize that X does not scale with N, 
as opposed to partial measures of complexity.In Figures 6 and 7, we show the dependence of X on N (on the plane) and on N and s (in threedimensions), respectively. We obtained the singularities of full complexity, Ncrj (s), j = 1, 2, as a resultof the zeroing of denominators in the Equation (17) at nonzero numerators.Note that, for M  Z, both measures of complexity have approximate valuesX(S; m0, n0) ≈ CX(S; m0, n0). Important differences between these two measures only appearfor Z/M close to 1, because only then does the denominator in Equation (17) play an important role.Of course, M is a free parameter, and possibly its specific value could be obtained from some additional(e.g., external) constraint.In Figure 4, we compare the behaviour of the partial (black curve) and full (orange curve) measuresof complexity, where we used the entropy instead of the specific entropy. Whether CX lies below orabove X depends both on M parameter (determining the weight at which individual measures ofpartial complexity enter the full measure of complexity), and on the Z/M ratio.Figure 4. Comparison of the partial measure of complexity CX(S; m = 2, n = 2) given by Equation (1)and full measure of complexity X(S; m0 = 2, n0 = 2) given by Equation (17), for instance, for thesymmetric case of m = n = m0 = n0. In addition, we assume that Smin = 0, Smax = 8 andM = 10. Vertical dashed lines indicate inflection points: black for the CX curve, orange for theX curve, while SmaxCX = SmaxX = 4. Notably, SmaxX maximizes X (here at a given ratio Z/M = 0.8).Vertical dashed lines mark the locations of inflection points on both curves.We continue to determine the full entropic susceptibility of the full measure of complexity,χX(S; m0, n0) =dX(S; m0, n0)dS= (m0 + n0)(SmaxCX − S)X(S; m0 − 1, n0 − 1)+2M2X(S; m0, n0)S − Sarit(1 − Smax−SM) (1 − S−SminM) , m0, n0 ≥ 1, (19)where SmaxCX is given here by Equation (4) but for m = m0 and n = n0. Notably, for the symmetric casesm = n and/or m0 = n0, we have SmaxCX = SmaxX = Sarit, which are independent of m, m0.Similarly to the partial entropic susceptibility of a partial measure of complexity, we obtain thefull entropic susceptibility of a full measure of complexity,χX(Ns; m0, n0) =dX(S; m0, n0)dS= (m0 + n0) N (smaxCX − s)X(Ns; m0 − 1, n0 − 1)+2M2X(Ns; m0, n0)Ns − sarit(1 − NM (smax − s)) (1 − NM (s − smin)) , m0, n0 ≥ 1, (20)13Entropy 2020, 22, 866where smaxCX = SmaxCX /N, sarit = Sarit/N, smin = Smin/N, and smax = Smax/N. The progression ofsusceptibility χX(S; m0, n0), depending on S, for selected parameter values is shown in Figure 5.This progression course is similar to the analogous one that is presented in Figure 3.Figure 5. Plot of the full entropic susceptibility χX(S; m0, n0) of the full measure of complexity vs. Sgiven by Equation (19), at arbitrary fixed order (m0 = 2, n0 = 2). As expected from the comparisonwith Figure 3, the turning points of CX (cf. Figure 4) lie within the S interval bounded by inflectionpoints of X.Thus, the evolution of X is governed by an equation that is analogous to Equation (6), except thatχCX present in that equation should be replaced by χX given by Equation (19). Therefore, we havedX(S(t), m0, n0)dt= χX(S(t); m0, n0)dS(t)dt. (21)The relationship between measures of complexity and time is implicit in our work—complexityindirectly depends on time through the dependence of entropy on time. It should be emphasized thatthe dependence of entropy on time is external in our approach—it can be taken into account based onadditional modelling that is dedicated to specific real situations. We have already signalled this whendiscussing Equation (6).2.7. Criticality in Extensive SystemsBy using Equation (17), we show when the universal full measure of complexity diverges and,thus, the system enters a critical state. We assume that we are dealing with an extensive system,i.e., that Equation (17) can be represented asX(Ns; m0, n0) =(1 − 1M)2Nm0+n0(smax − s)m01 − NM (smax − s)× (s − smin)n01 − NM (s − smin),NzM< 1, (22)where entropy densities s(= S/N), smin(= Smin/N), smax(= Smax/N) are (at most) slowly varyingfunctions of the number N of elements making up the system and special entropy span z = smax − smin.As one can see, the measure X is divergent in two critical points Nmaxcr (s) =Msmax−s and Nmincr (s) =Msmin−s ,14Entropy 2020, 22, 866where smin < s < smax. Moreover, the susceptibilities given by Equations (19) and (20) diverge at thesame points where measures of complexity given by Equations (17) and (18) diverge, which underlinesthe self-consistency of our approach.Equation (22) can now be written in a form that explicitly includes both critical points (bothphysical and non-physical):X(Ns; m0, n0) =(1 − 1M)2Nm0+n0(smax − s)m0(1 − NNmaxcr (s))βmax × (s − smin)n0(1 − NNmincr (s))βmin , (23)where critical exponents assume the mean-field values βmax = βmin = 1. In this case, we could speakof two-criticality were it not for the fact that one of these criticalities is unphysical.Figure 6 shows dependence X(Ns; m0, n0) vs. N at fixed s = 0.8. The values of parameters areshown there, while the specific entropy s is chosen so that the condition s − smin < smax − s is satisfied(this is equivalent to a condition s < sarit). This means that s is closer to smin than smax. The existenceof these divergences is a signature of criticality. However, the situation for borderline cases s = smin ors = smax changes rapidly—it is a different consideration.Critical numbers of entities in the system Nmaxcr (s) and Nmincr (s) are determined by the ratioof the M parameter characterizing the hierarchy/cascade of scales in the system and the distancebetween entropy density s and its extreme values smin and smax. The construction of these criticalnumbers resembles the canonical critical temperature structure for the Ising model in the mean-fieldapproximation, where βc Jz = 1 (here βc = 1/kBTc and kB is the Boltzmann constant). In our case,the role of the inverse temperature βc is played by Nmaxcr and Nmincr , the role of the coupling constant J is1/M, while the role of the mean coordination number z is played by smax − s and s − smin, respectively.The hierarchy is the source of criticality here. Criticality is an immanent feature of ourfull description of complexity. Nevertheless, in this work, we do not specify the sources of thishierarchy—it could be self-organized criticality or due to some other sources.For the sake of completeness, note that the dependence on N of the partial measure of complexityis given by Equation (2). This means that for extensive systems this measure increases powerfullydepending on N. Therefore, only the weighted infinite sum of these measures generates the existenceof singularity.Figure 6. Dependence of the universal full measure of complexity X vs. number of entities N givenby Equation (23). It should be emphasized that the full measure of complexity and its susceptibilityhave singularities in the same points. As one can see, we are dealing here with complexity barriersseparating the phases/states of the system and the small and large number of objects forming them.The parameters we adopted here are as follows: M = 30, smin = 0, smax = 2, s = 0.8, m0 = n0 = 2,hence, point Nmaxcr (s = 0.8) = 25 and point Nmincr (s = 0.8) = 37.5.15Entropy 2020, 22, 866Let us now consider in more detail the behaviour of X(Ns; m0, n0) depending on N and s.A three-dimensional plot of Figure 7 will be helpful here. One can see how the mutual location of thesingularities of Nmaxcr (s) and Nmincr (s) changes with the increase of s. From the situation of s < sarit,in which Nmaxcr (s) > Nmincr (s), through the situation when s = sarit in which Nmaxcr (s) = Nmincr (s), up tothe situation in which Nmaxcr (s) > Nmincr (s) for s > sarit.It must be clearly stated that the area physically accessible is the one in front of the first singularity,which is further emphasized in Figure 7 by blue curves. Let us emphasize that the N range in whichcriticality occurs is sufficient to cover the corresponding values of N discussed in the literature to date,especially the Dunbar numbers [46–49] (e.g., N = 5, 15, 50, and N = 150). However, it should be notedthat our view of complexity is complementary to that presented in the literature.Figure 7. Dependence of the universal full measure of complexity X vs. number of entities N andspecific entropy s given by Equation (23), for m0, n0 ≥ 1. Notably, the full measure of complexityand its susceptibility have singularities at the same points Nmaxcr (s) and Nmincr (s). We are dealinghere with complexity barriers separating the phases/states of the system and the small and largenumber of entities that form them. The parameters we adopted here are, as follows: M = 30,smin = 0, smax = 2, s = 0.8, m0 = n0 = 2. These are the same parameters that we used to construct theplain plot in Figure 6.3. Finger Print of Complexity in SimplicityLet us consider a perfect gas at a fixed temperature, which is initially closed in the left half of anisolated container. The partition is next removed, and the gas undergoes a spontaneous expansion.Here we are dealing (practically speaking) with an irreversible process, even for a small number ofparticles (at least the order of 102).Let us recall the definition of ’perfect gas’. It is a gas of particles that cannot ‘see’ each other,i.e., there are no interactions between them. Thus, from a physical point of view, it is a dilutegas at high temperature. We further assume that all of the particles have the same kinetic energy.A legitimate question is whether such a gas will expand after the partition is removed. We noticethat the thermodynamic force is at work here, being roughly proportional to the difference in thenumber of particles in the right and left parts of the container. This force causes the expansion process.Thus, we are dealing with the simplest paradigmatic irreversible process [50]. The particles remainstuck in the final state and will not leave it (with accuracy subject to slight fluctuations in the numberof particles in the right half of the container). Such a final state of the whole system is referred to as theequilibrium state. The simple coarse-grain description of the system allows us to introduce here theconcept of configuration entropy.Note that the macroscopic state of the system (generally, the non-equilibrium andnon-stationary/relaxing one) can be described by the instantaneous number of particles in the left16Entropy 2020, 22, 866(NL(t)) and right (NR(t)) parts of the container, with N = NL(t) + NR(t), where N is the fixed totalnumber of particles in the container (isolated system). It allows for one to define the weight of themacroscopic state Γ(NL(t)), also called thermodynamic probability. This is the number of ways toarrange the NL(t) particles in the left part of the container and NR(t) = N − NL(t) in the right. Hence,Γ(NL(t)) =N!NL(t)!(N − NL(t))! . (24)Here we do not distinguish permutations of particles inside each part of the container separately.We only take into account permutations of particles located in different halves of the container. This isbecause our resolution here is too small to observe the location of particles inside each containerseparately. Such a coarse-graining creates an information barrier: more information can mask thecomplexity of the system. We will not be able to see the complexity, because we will not be able toconstruct entropy. This creates a paradoxical situation: the surplus of information makes the taskdifficult and does not facilitate obtaining the insight into the system. Here we have an analogy withchaotic dynamics, where chaos is only visible in the Poincaré surface cross-section of the phase spaceand not in the entire phase space.The configuration entropy at a given time t we define, as follows,S(NL(t)) = ln Γ(NL(t)), (25)where Γ(NL(t)) is given by Equation (24). The above expression can be used both for the equilibriumand non-equilibrium states.It can be demonstrated using the Stirling formula that for large N, entropy S is reduced to theBGS form,ln Γ(NL(t)) = −N [pL(t) ln pL(t) + pR(t) ln pR(t)] = Ns(t), (26)where pJ(t)def.=NJ(t)N , J = L, R, and s(t) is a specific entropy. The law of entropy increase Equation (A8)is also fulfilled here, as expected.We now prepare the equation for determining NCXmaxL , i.e., the number of particles in the left part ofthe container that maximizes the partial complexity measure CX. To this end, we assume, for instance,the symmetric partial measure of complexity of the order of (m = 2, n = 2). Next, we substituteNL = NCXmaxL into the both sides of Equation (25) and according to constitutive Equation (5), we equateEquation (25) to SmaxCX . Hence, we obtain a constitutive equation for the relaxing perfect gas,S(NL(t) = NCXmaxL ) = SmaxCX , (27)where NCXmaxL is our sought quantity.Now, we need to independently determine SmaxCX . Recall that the number of NL particles thatmaximize entropy is the number of NeqL particles in the statistical/thermodynamic equilibrium state ofthe system. This number is equal to half of all particles in the container, i.e., NeqL = N/2. It can still beassumed (without reducing the general considerations) that Smin = 0. Therefore,Smax = S(N/2). (28)However, from Equation (A5), we know that SmaxCX = Smax/2. By using it, we transformEquation (27) into the form,S(NCXmaxL)=12S(N/2). (29)Equation (27) is an example of the general constitutive Equation (5), where NCXmaxL plays the roleof YCXmax. This equation has the following explicit form,17Entropy 2020, 22, 866[ΠN−NCXmaxLj=1(1 +NCXmaxLj)]2= ΠN/2j=1(1 +N2j), for n = m = 2. (30)Just deriving Equation (30) (see Appendix C for details) is the primary purpose of this example.This is a transcendental equation of which the exact analytical solution is unknown. When derivingEquation (30), we used the initial condition for the entropy that is, S(t = 0) = Smin = ln Γ(NL = N) = 0,which follows from Equations (24) and (25). Even for such a simple toy model, determining the partialmeasure of complexity is a non-trivial task, also because NL is different from N/2 (as we show below).The numerical solutions of Equation (30), i.e., the relationship of NCXmaxL to N, are shown inFigure 8 (for simplicity, L defining the vertical axis on the plot means NCXmaxL ). Both of the solutions(small circles above and below the solid straight line) show that NCXmaxL is significantly different fromN/2. Thus, the most complex state is significantly different from the equilibrium state.Figure 8. Dependence of L(= NCXmaxL ) vs. N. There are two solutions of Equation (30): one markedwith blue circles and the other with orange ones. Above N ≈ 102, both dependencies are linear, whichis particularly clearly confirmed in Figure 9. That is, in a log-log scale, their slopes equal 1. However,in linear scale, the directional coefficients of these straight lines equal 0.11 and 0.89, respectively. This isclearly shown in Figure 9. Only the solution with orange circles is realistic, because the chance that 89%of particles will pass in a finite time to the second part of the container (as indicated by the solutionmarked with blue circles) is negligibly small. The black solid tangent straight line indicates a referencecase NCXmaxL = N/2.Figure 9. Directional coefficient of linear dependencies L vs. N as a function of N. For N greaterthan 102, no N-dependence of this coefficient is observed. Both of the solutions (having L/N = 0.11and L/N = 0.89) are mutually symmetric about the straight horizontal line L/N = 1/2, but we onlyconsider the solution L/N = 0.89 to be realistic. The black horizontal straight solid line indicates areference case NCXmaxL = N/2.18Entropy 2020, 22, 866Having the NCXmaxL dependence on N, we can obtain the dependence of the partial measure ofspecific complexity cxmax def.= CXmax/Nm+n on N order (m = 2, n = 2). We can writecxmax =(s(N/2)2)4=[12Nln(ΠN/2j=1(1 +N2j))]4, (31)as in our case smax = s(N/2) equals the logarithm of the right-hand side of Equation (30) divided byN. Notably, Equation (31) is based on Equation (A12).In Figure 10, we present the dependence of cxmax on N. Quantity cxmax is a non-extensivefunction—it reaches the plateau for N  1. For N ≈ 104 the plateau is achieved with a goodapproximation. This is important for researching 
complexity. Namely, systems can